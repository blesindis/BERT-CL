import torch
import torch.nn as nn
from transformer.Transformer import BertModel, BertMOEModel, SwitchBertModel
from transformer.MoT import BertMOTModel
from transformer.MoTSwitch import SwitchMoTModel
from transformer.MoTAttn import BertMOTAttnModel
from transformer.MoTAttnAuto import BertMOTAttnAutoModel
from transformer.MoTAttnLoRA import BertMOTAttnLoRAModel
from transformer.MoT_warmup import BertMOTWarmupModel
from transformer.MoTAttnToken import BertMOTAttnTokenModel
from transformer.ReverseMoE import SwitchReverseModel
from transformers.models.bert.modeling_bert import BertOnlyMLMHead


# origin bert
class BertForMLM(nn.Module):
    def __init__(self, config):
        super(BertForMLM, self).__init__()
        self.config = config
        self.bert = BertModel(config)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertMOTAttnAuto(nn.Module):
    def __init__(self, config):
        super(BertMOTAttnAuto, self).__init__()
        self.config = config
        self.bert = BertMOTAttnAutoModel(config)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertWithMoT(nn.Module):
    def __init__(self, config, centers):
        super(BertWithMoT, self).__init__()
        self.config = config        
        self.bert = BertMOTModel(config, centers)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertWithMoTAttn(nn.Module):
    def __init__(self, config, centers):
        super(BertWithMoTAttn, self).__init__()
        self.config = config        
        self.bert = BertMOTAttnModel(config, centers)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)
        # self.apply_random_mask()

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
            
    def apply_random_mask(self):
        """No embedding & decoder"""
        # total_params = sum(p.numel() for p in self.bert.layers.parameters())
        """All"""
        total_params = sum(p.numel() for p in self.parameters())
        num_masked = total_params // 2

        # Create a flat mask with 50% ones and 50% zeros
        flat_mask = torch.cat([
            torch.ones(num_masked),
            torch.zeros(total_params - num_masked)
        ])
        # Shuffle the mask
        flat_mask = flat_mask[torch.randperm(total_params)]

        idx = 0
        """No embedding & decoder"""
        # for param in self.bert.layers.parameters():
        # """All"""
        for param in self.parameters():
            param_numel = param.numel()
            param_mask = flat_mask[idx:idx + param_numel].reshape(param.shape)
            param.data.mul_(param_mask)
            idx += param_numel

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertWithMoTAttnLoRA(nn.Module):
    def __init__(self, config, centers):
        super(BertWithMoTAttnLoRA, self).__init__()
        self.config = config        
        self.bert = BertMOTAttnLoRAModel(config, centers)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)
        # self.apply_random_mask()

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
            
    def apply_random_mask(self):
        """No embedding & decoder"""
        # total_params = sum(p.numel() for p in self.bert.layers.parameters())
        """All"""
        total_params = sum(p.numel() for p in self.parameters())
        num_masked = total_params // 2

        # Create a flat mask with 50% ones and 50% zeros
        flat_mask = torch.cat([
            torch.ones(num_masked),
            torch.zeros(total_params - num_masked)
        ])
        # Shuffle the mask
        flat_mask = flat_mask[torch.randperm(total_params)]

        idx = 0
        """No embedding & decoder"""
        # for param in self.bert.layers.parameters():
        # """All"""
        for param in self.parameters():
            param_numel = param.numel()
            param_mask = flat_mask[idx:idx + param_numel].reshape(param.shape)
            param.data.mul_(param_mask)
            idx += param_numel

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertWithMoTAttnToken(nn.Module):
    def __init__(self, config, centers):
        super(BertWithMoTAttnToken, self).__init__()
        self.config = config        
        self.bert = BertMOTAttnTokenModel(config, centers)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)
        # self.apply_random_mask()

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
            
    def apply_random_mask(self):
        """No embedding & decoder"""
        total_params = sum(p.numel() for p in self.bert.layers.parameters())
        """All"""
        # total_params = sum(p.numel() for p in self.parameters())
        num_masked = total_params // 5

        # Create a flat mask with 50% ones and 50% zeros
        flat_mask = torch.cat([
            torch.ones(num_masked),
            torch.zeros(total_params - num_masked)
        ])
        # Shuffle the mask
        flat_mask = flat_mask[torch.randperm(total_params)]

        idx = 0
        """No embedding & decoder"""
        for param in self.bert.layers.parameters():
        # """All"""
        # for param in self.parameters():
            param_numel = param.numel()
            param_mask = flat_mask[idx:idx + param_numel].reshape(param.shape)
            param.data.mul_(param_mask)
            idx += param_numel

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertMOTWarmup(nn.Module):
    def __init__(self, config):
        super(BertMOTWarmup, self).__init__()
        self.config = config
        self.bert = BertMOTWarmupModel(config)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
    
class BertWithMOE(nn.Module):
    def __init__(self, config, centers):
        super(BertWithMOE, self).__init__()
        self.config = config        
        self.bert = BertMOEModel(config, centers)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertSwitch(nn.Module):
    def __init__(self, config):
        super(BertSwitch, self).__init__()
        self.config = config
        self.bert = SwitchBertModel(config)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertMoTSwitch(nn.Module):
    def __init__(self, config):
        super(BertMoTSwitch, self).__init__()
        self.config = config
        self.bert = SwitchMoTModel(config)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores
    
    
class BertSwitchReverse(nn.Module):
    def __init__(self, config):
        super(BertSwitchReverse, self).__init__()
        self.config = config
        self.bert = SwitchReverseModel(config)
        self.head = BertOnlyMLMHead(config)
        self.criterion = nn.CrossEntropyLoss() 
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, input_ids, attention_mask, labels):
        output = self.bert(input_ids, attention_mask)
        scores = self.head(output)
        mlm_loss = self.criterion(scores.view(-1, self.config.vocab_size), labels.view(-1)) # scores should be of size (num_words, vocab_size)

        return mlm_loss, scores